---
title: 'Problem Set #5'
author: "Cian Stryker"
date: "10/11/2019"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Here we go again with a loading chunk. Basic stuff here to be honest. I've used a few new packages like fs for my first two questions.

library(readr)
library(readxl)
library(janitor)
library(gt)
library(fs)
library(scales)
library(moderndive)
library(googlesheets4)
library(data.table)
library(cowplot)
library(infer)
library(styler)
library(tidyverse)
```

## Question 1
```{r, Loading for Question 5, warning=FALSE, message=FALSE}

# This is all new because I hadn't done this on my last problem set. First step is to create a directory.

dir_create("raw-data")

# Now I'm downloading the raw data and then I'm sending it to my new directory.

download.file("https://raw.githubusercontent.com/TheUpshot/2018-live-poll-results/master/data/elections-poll-oh01-1.csv", destfile = "C:/Users/cians/OneDrive/Documents/ps-5-release-CianStryker/raw-data/ohio_polls", mode = "wb")

# Now I'm just loading it up and cleaning it up with janitor

polls_ohio <- read_csv("raw-data/ohio_polls") %>%
  clean_names()
```

```{r, Plot 1 for Question 5, warning=FALSE, message=FALSE}

# Alright, I know what you're thinking. "Why has he repeated this code so many time? It's so redundant". I get it. But I couldn't figure out how to replicate the code differing the filter command, so I had to do it this way. If you know how to simplify this, please let me know.

# Anyway... I wanted to show my hometown's voting district's voting preferences, but demonstrating how it's been gerrymandered to exclude Black neighborhoods, and how that has an effect on the voting distric't voting preferences. So I had to first find the overall voting preferences, before exploring the demographic breakdown, and then exploring White and Black voting preferences. Step one was overall the demographic breakdown though. I just grabbed what I needed first.

start <- polls_ohio %>%
  select("file_race", "partyid")

# I don't want to hear it. Yeah I repeat this filter command four times. I wanted to find out how many Asian, Black, Hispanic, and White voters there were, as these are the four major racial groups. A simple filter and count sequence does this. I assing each result to its own variable. Oh, I'm not sure why by nrow() threw up a ton of issues, so I used count.

asian <- start %>%
  filter(file_race == "Asian") %>%
  count()
black <- start %>%
  filter(file_race == "Black") %>%
  count()
hispanic <- start %>%
  filter(file_race == "Hispanic") %>%
  count()
white <- start %>%
  filter(file_race == "White") %>%
  count()

# Now I combine all four into one data frame with the always useful data.frame command. Also I melt them so I can actaully graph them.

d <- data.frame(asian, black, hispanic, white) %>%
  melt()

# Graphing time again. Don't get too excited though, because I do this three more times. Pretty simple work here on my end. I just made a bar chart because I wanted some diversity in my graphs and I had already decided to make some pie charts later. A little label work, some fill for color, and then expanding limits to make it looks better did the trick for the plot.

plot <- ggplot(d, aes(x = variable, y = value, fill = variable)) +
  geom_col() +
  scale_x_discrete(labels = c("Asian", "Black", "Hispanic", "white")) +
  labs(
    title = "Racial Breakdown",
    subtitle = "Ohio's First Voting District",
    x = " ",
    y = " "
  ) +
  expand_limits(y = 400) +
  guides(fill = FALSE)
```

```{r, Plot 2 for Question 5, warning=FALSE, message=FALSE}
# I'm going to start sounding very repetetive from here on, so I'm sorry. Now I want to find overall voting preference in the voting district. So I grab partyid.

start5 <- start %>%
  select("partyid")

# Yeah I did it again. Very repetetive, but stack exchange didn' teach me how to make a function for this so whatever. I'm filtering for the three major political affiliations: Dem, Ind, and Rep. I'm using nrow to count them and then registering them as variables.

d5 <- start5 %>%
  filter(partyid == "Democrat") %>%
  nrow()
i5 <- start5 %>%
  filter(partyid == "Independent (No party)") %>%
  nrow()
r5 <- start5 %>%
  filter(partyid == "Republican") %>%
  nrow()

# Who could've expected that I'd combine them into one data.frame and melt them? Wow. Crazy stuff Cian. But yeah that's what I did.

e5 <- data.frame(d5, i5, r5) %>%
  melt()

# I wanted to perfectly copy the previous graph so I literally did the exact same thing with different labels. Oh, I did do a scale_fill_manual command to make the colors match the political affiliations. That's kinda cool right?

plot5 <- ggplot(e5, aes(x = variable, y = value, fill = variable)) +
  geom_col() +
  scale_x_discrete(labels = c("Democrat", "Independent", "Republican")) +
  scale_fill_manual(values = c("blue", "grey", "red")) +
  labs(
    title = "Party Affiliation",
    subtitle = "Ohio's First Voting District",
    x = " ",
    y = " "
  ) +
  expand_limits(y = 400) +
  guides(fill = FALSE)
```

```{r, Plot 3 for Question 5, warning=FALSE, message=FALSE}

# Yeah... its happening again. I wanna see voting preference among white voters, so I had to grab just white voters in the district.

start2 <- start %>%
  filter(file_race == "White") %>%
  select("partyid")

# What's he doing here? I think you probably get it by  now. Using the filter and nrow commands to grab what I want, count them, and then register them as variables.

d <- start2 %>%
  filter(partyid == "Democrat") %>%
  nrow()
i <- start2 %>%
  filter(partyid == "Independent (No party)") %>%
  nrow()
r <- start2 %>%
  filter(partyid == "Republican") %>%
  nrow()

# It's data.frame and melting time! Practice makes perfect so I'm just establishing mastery over these commands.But they are necessary for creating a data.frame that is graphable and a combination of the data I need.

e <- data.frame(d, i, r) %>%
  melt()

# Now I'm making a pie chart. They're not great charts for visualizing information, but I hadn't made them before so I wanted to. It's actually super easy. It's just a geom_bar() with a coord_polar() bit added in. I did remove some the graphing marks around the perimeter though. I made good labels and manually matched each party with their color. Also, I'm going to admit this because I have to, I just manually calculated the percentages and added them into the legend labels for each party. I don't like it when numbers are on the pie chart. It's a weird personal quirk.

plot1 <- ggplot(e, aes(x = "", y = value, fill = variable)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  labs(
    title = "Party ID amongst White Voters",
    subtitle = "Ohio's First Voting District",
    x = " ",
    y = " "
  ) +
  scale_fill_manual(name = "Party", labels = c("Democrat-- 22%", "Independent-- 36%", "Republican-- 42%"), values = c("blue", "grey", "red"))
```

```{r, Plot 4 for Question 5, warning=FALSE, message=FALSE}

# I'm running out of steam here, but yeah I just want to see preference among Black voters so I just grabbed them through a filter and select sequenc.

start3 <- start %>%
  filter(file_race == "Black") %>%
  select("partyid")

# I don't even have energy for sarcasm at this point. I'm really hoping someone will show me how to replicate a filter command, but until then this is how I'll handle this. I ran the filter and nrow commands for each political party to create four variables.

d2 <- start3 %>%
  filter(partyid == "Democrat") %>%
  nrow()
i2 <- start3 %>%
  filter(partyid == "Independent (No party)") %>%
  nrow()
r2 <- start3 %>%
  filter(partyid == "Republican") %>%
  nrow()

# Here I'm combining them and then melting them into long form.

e2 <- data.frame(d2, i2, r2) %>%
  melt()

# Final graph. It looks exactly like the previous pie chart, but I did add a caption section. I thought it would be good to show where the data came from for the whole plot. Since this is the last graph in the four part series, I added it to this once, so it would show up in the bottom.

plot2 <- ggplot(e2, aes(x = "", y = value, fill = variable)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  labs(
    title = "Party ID amongst Black Voters",
    subtitle = "Ohio's First Voting District",
    x = " ",
    y = " ",
    caption = "Source: Data  from the New York Time's 2018 Polling"
  ) +
  scale_fill_manual(name = "Party", labels = c("Democrat-- 59%", "Independent-- 30%", "Republican-- 11%"), values = c("blue", "grey", "red"))
```

```{r, Final Group Plot for Question 5}

# Finally something new! So I really like cowplot and I wanted to show each graph together in one place. With cowplot you can see how my graphs make a point. The first bar chart shows that in the voting distric there is a slight advantage for the republican party, but it isn't necessarily signifigant. Then in the second graph you can see that the voting district is overwhelmingly white with the next largest racial group being African Americans. Then with the pie charts you can see that White voters mostly lean towards the republican party, but Black voters predominantly vote democrat. Now this graph series is even more interesting if you google the actual voting district boundaries. It is very strangely drawn and avoids several well known black neighborhoods. For me this graph series shows a delibrate strategy in Cincinnati and also demonstrates why Representative Chabot has resisted being removed from office even though Cincinnati is very liberal leaning. By excluding Black voters from his district, he can capitalize on the white population that is just republican enough to keep him in office.

plot_grid(plot5, plot, plot1, plot2,
  labels = "AUTO",
  label_size = 12
)
```


## Question 2 
http://rpubs.com/Cian401/question_2



## Question 3

```{r, Question 3 Loading, warning=FALSE, message=FALSE}

# Gotta deauthorize the process to make life easier.

sheets_deauth()

# Just loading the necessary sheet up so I can start the question.

data <- sheets_read("https://docs.google.com/spreadsheets/d/14Zxd1-xdUoLfaCG6j55T9W2WvqchyBGJy3J2sAwTXPM/edit#gid=480043959")
```


```{r, Question 3, message=FALSE}

# We were told to add this bit in. Not really sure what it does though

set.seed(9)

# First step is to just to recreate the infer workflow that was used in the reading. I really like infer so I opted to use it instead of rep_sample_n(), but generally I'm just finding the mean of republican cases over 1000 replications.

bootstrap_1 <- data %>%
  specify(response = income) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean")

# Here I'm just grabbing my confidence interval for both 95 and 2.5. Really like infer.

results <- bootstrap_1 %>%
  get_confidence_interval(level = 0.95, type = "percentile")

# I'm keeping these four code lines together because it's easier to write about them as a group. First I'm rounding the results, then I'm adding a nice "," as my big mark then I'm making it a data.frame, and finally I'm pasting in a "$" in front of them. Note: this makes it a character type, but at this stage that's fine.

results <- round(results)
results2 <- prettyNum(results, big.mark = ",")
results3 <- data.frame(results2)
results3$results2 <- paste0("$", results3$results2)

# Easy head command to grab what I want, which happens to be the first observations.

a <- results3 %>%
  head(1)
```

The 2.5% percentile of average income is `r a`.
```{r, Question 3 part 2}

# Instead of using head, I used tail because the second madlib is just my second and last observation.

b <- results3 %>%
  tail(1)
```

The 97.5% percentile of average income is `r b`.


## Question 4

```{r, Plot part 1, warning=FALSE, message=FALSE}

# Okay first step is just to create our five separate groups from the data that we have. Nothing crazy here, but I wonder if I could've made the code simpler? Maybe, but this is what I'm doing for now. Also I'm just selecting out the republcian column to make life easier for later one.

group_1 <- data[1:20, 1:6] %>%
  select("republican")
group_2 <- data[21:40, 1:6] %>%
  select("republican")
group_3 <- data[41:60, 1:6] %>%
  select("republican")
group_4 <- data[61:80, 1:6] %>%
  select("republican")
group_5 <- data[81:100, 1:6] %>%
  select("republican")


# So yeah this is redundant coding, but seriously I don't know how to use purrr well enough to make this simpler. I'll ask at study hall at somepoint, but since using purrr isn't a requirement of this question, I'm doing it the way I know. I'm essentially finding out the mean 1000 times via the bootstrap method for each of my previous five groups. Then I'm grabbing just the column "stat". Also for each group I'm renaming the column stat to Group for later aesthetic purposes.

g_1 <- group_1 %>%
  specify(response = republican) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean") %>%
  select("stat")
colnames(g_1)[colnames(g_1) == "stat"] <- "Group 1"
g_2 <- group_2 %>%
  specify(response = republican) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean") %>%
  select("stat")
colnames(g_2)[colnames(g_2) == "stat"] <- "Group 2"
g_3 <- group_3 %>%
  specify(response = republican) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean") %>%
  select("stat")
colnames(g_3)[colnames(g_3) == "stat"] <- "Group 3"
g_4 <- group_4 %>%
  specify(response = republican) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean") %>%
  select("stat")
colnames(g_4)[colnames(g_4) == "stat"] <- "Group 4"
g_5 <- group_5 %>%
  specify(response = republican) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean") %>%
  select("stat")
colnames(g_5)[colnames(g_5) == "stat"] <- "Group 5"

# Now I'm making a data.frame of each group to bring them together, then I'm melting them down to long form, and finally I'm multiplying by 100 to get rid of the decimal percentage form. Whole numbers are just definitivley better.

groups <- data.frame(g_1, g_2, g_3, g_4, g_5)
groups_x <- reshape2::melt(groups) %>%
  mutate(percent = value * 100)
```

```{r, Plot}

# Nothing crazy here. It's a histogram with a facet wrap. I added in some titles and played arounnd with the biwidth until it looks better. I added in some percent signs too.

ggplot(groups_x, aes(x = percent, color = "red", fill = "red")) +
  geom_histogram(binwidth = 5) +
  facet_wrap(~variable) +
  labs(
    title = "Distribution of Republicans per Sample of Commuters",
    x = "Percent",
    y = "Count"
  ) +
  scale_x_continuous(
    breaks = c(0, 20, 40, 60),
    labels = c("0%", "20%", "40%", "60%")
  ) +
  guides(color = FALSE, fill = FALSE)
```

```{r, Table part 1, warning=FALSE, message=FALSE}

# Essentially I'm doing what I did before but now I don't want to select just the stat column, so I'm repeating my former code lines with that last part removed. I also rename them so I can work with them more easily.

g_1x <- group_1 %>%
  specify(response = republican) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean")
g_2x <- group_2 %>%
  specify(response = republican) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean")
g_3x <- group_3 %>%
  specify(response = republican) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean")
g_4x <- group_4 %>%
  specify(response = republican) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean")
g_5x <- group_5 %>%
  specify(response = republican) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean")

# First I'm calculating my confidence intervals for each group.

intervals1 <- g_1x %>%
  get_confidence_interval(level = 0.95, type = "percentile") 
intervals2 <- g_2x %>%
  get_confidence_interval(level = 0.95, type = "percentile") 
intervals3 <- g_3x %>%
  get_confidence_interval(level = 0.95, type = "percentile") 
intervals4 <- g_4x %>%
  get_confidence_interval(level = 0.95, type = "percentile") 
intervals5 <- g_5x %>%
  get_confidence_interval(level = 0.95, type = "percentile") 

#Now I'm grabbing the 2.5% info.

intervals1_x <- intervals1 %>%
  select("2.5%")
intervals2_x <- intervals2 %>%
  select("2.5%")
intervals3_x <- intervals3 %>%
  select("2.5%")
intervals4_x <- intervals4 %>%
  select("2.5%")
intervals5_x <- intervals5 %>%
  select("2.5%")

#Finally I'm grabbing the 97.5% column. I'm sorry

intervals1_y <- intervals1 %>%
  select("97.5%")
intervals2_y <- intervals2 %>%
  select("97.5%")
intervals3_y <- intervals3 %>%
  select("97.5%")
intervals4_y <- intervals4 %>%
  select("97.5%")
intervals5_y <- intervals5 %>%
  select("97.5%")

# Now I'm just combining them together in a data frame, melting them into long form, creating a percent column by multiplying the data column by 100, and then finally I select just the percent column I made.

part_1 <- data.frame(intervals1_x, intervals2_x, intervals3_x, intervals4_x, intervals5_x) %>%
  reshape2::melt() %>%
  mutate(percent = value * 100) %>%
  select("value")

part_1x <- data.frame(intervals1_y, intervals2_y, intervals3_y, intervals4_y, intervals5_y) %>%
  reshape2::melt() %>%
  mutate(percent = value * 100) %>%
  select("value")

# Here I'm just finding the overall mean for each of the groups I created two code sections above. Again, yeah it's repetetive but it works and isn't actually that much work at this stage. I can't get any list functions to work for it so far though.

mean_1 <- g_1x %>%
  summarize(est_mean = mean(stat))
mean_2 <- g_2x %>%
  summarize(est_mean = mean(stat))
mean_3 <- g_3x %>%
  summarize(est_mean = mean(stat))
mean_4 <- g_4x %>%
  summarize(est_mean = mean(stat))
mean_5 <- g_5x %>%
  summarize(est_mean = mean(stat))

# Here I'm just combining my means, melting them into long form, and then grabbing the value column, which cooresponds to the mean information.

part_2 <- data.frame(mean_1, mean_2, mean_3, mean_4, mean_5) %>%
  reshape2::melt() %>%
  select("value")

# I create a column of my group names, which will make the actual table creation much easier.

reorder <- c("Group 1", "Group 2", "Group 3", "Group 4", "Group 5")

# Now I merge everything together and it's almost in the perfect format to be made into a table.

final_part <- data.frame(reorder,part_1, part_1x, part_2)
```

```{r, Table, warning=FALSE}

# Since all the heavy work is already done, I just use the gt package to make the table. I add in some percent sines to make the mean column fancier, but I elected to keep the confidence interval section in decimal form. I thought it looked better to be honest. I center my columns and add in enough labels to make it look logical.

gt(final_part) %>%
  fmt_percent(vars(value, value.1, value.2), decimals = 1) %>%
  cols_label(reorder = " ", value = "2.5%", value.1 = "97.5%", value.2 = "Overall Mean") %>%
  tab_header(
    title = "Distribution of Samples",
    subtitle = "Commuters"
  ) %>%
  tab_spanner(
    label = "95th Confidence Interval",
    columns = vars("value", "value.1")) %>%
  cols_align(align = "center")
```


In the plot above we see that the 95th confidence intervals and overall means of each group differs. This demonstrates the weaknesses of a small sample size. Within a small sample size, in this case n = 20, even if we replicate with replacment many times, there is a very high chance for the data to be skewed in one way or the other. This is because outlier data has far more potential to disrupt overall distribution when the sample size is smaller, whereas in a larger sample size the relative effect of an outlier is diminshed by the weight of the mean tendency. 

## Question 5
```{r, dice functions}
knitr::opts_chunk$set(echo = TRUE)

# We did this in class, so I've just loaded it over.

dice_roll <- function() {
  Dice <- c(1:6)
  sample(Dice, 1)
}

# Same as the previous explanation. Both of these function are necessary for tackling question 5.

dice <- function(n = 1) {
  results <- vector(mode = "integer", length = n)

  for (i in 1:n) {
    results [i] <- dice_roll() + dice_roll()
  }
  return(results)
}
```

```{r, Question 5 a-d, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

#This is honestly hard for me to even explain, but I'll give it a shot. So step one for part a is creating the tibble that has a column that runs 1 through 10 that matches a second column that has a string of three dice rolls each. This forms the basis for moving forward. 

#So the goal here for part b is to create another column that reads either TRUE or FALSE if the first dice roll in the string column is 7. To do this I'm using mutate to create the variable "first_seven" which equals a map command that uses an anonymous function in function place. This function, which reads for throws (the string column), is composed of an if_else command that reads into a section that asks if the first string variable should be 7. Then it reads TRUE if so, or FALSE if not. The key here is that using two brackets like this [[]] denotes the column and then opens up the individual nested observation.

#Part c builds off part b. Essentially now I'm doing everyting again but I'm creating a new variable called a_winner that will read as TRUE if any number within an nested observation has a 7 or 11. Here I'm not usin if_else, but case_when, which lets me specify two separate arguments that signal for either 7 or 11. I'm not going to lie though, I'm not entirely sure why this even works.

x <- tibble(replication = 1:10, throws = map(1:10, ~ dice(n = 3))) %>%
  mutate(first_seven = map_lgl(throws, function(throws) {
    if_else(7 == throws[[1]], TRUE, FALSE)
  })) %>%
  mutate(a_winner = map_lgl(throws, function(throws) {
    case_when(
      any(c(7) %in% unlist(throws)) ~ TRUE,
      any(c(11) %in% unlist(throws)) ~ TRUE,
      TRUE ~ FALSE
    )
  }))

#Here I'm just runnign str on my last tibble to answer part d

str(x)
```

```{r,Question 5 e}
knitr::opts_chunk$set(echo = TRUE)

#So this is a continuation of the last code, but now for 10000 repetitions to answer part e. I also made a variable called perfection that should signal TRUE if every single interger in a nested observation is 7 or 11. The key here is to ifelse in conjuction with all, but separate 7 and 11 with a | command. Again though, I'm not sure why unlist is even necessary. I'll ask at some point if I have to. 

x2 <- tibble(replication = 1:10000, throws = map(1:10000, ~ dice(n = 3))) %>%
  mutate(perfection = map_lgl(throws, function(throws) {
    ifelse(all(7 == unlist(throws) | 11 == unlist(throws)), TRUE, FALSE)
  }))

#To answer the Madlib I had to create a percent column that is the sum of perfection (or the amount of TRUE's that pop up), and then divide by 10000 to find the percent. I used pull here to grab what I needed and list it as an integer.

y <- x2 %>%
  summarise(percent = sum(perfection) / 10000) %>%
  pull(1)

#Then it's just a matter of formatting it as a percent and then putting in an inline r code. 

y2 <- percent(y, suffix = "%")
```
Approximately `r y2` of the three rolls of a fair dice are all equal to either 7 or 11. 
```{r, Question 5 f}
knitr::opts_chunk$set(echo = TRUE)

#Alice told me to do this to get standard results....

set.seed(9)

#seriously? This entire question was wildly difficult but this was super tough. Luckily with enough perserverance and help we were able to troubleshoot through this. The idea heere is we need to create a function that will let us record results in a tibble. To begin with we just recreated the dice command syntax we did in class, but specified logical for the mode instead and set length to n so we could use it as a function later. 

#Then we made a basic tibble that rolls a pair of dice 10 times using a map_int function, whic his important becuase our column isn't a list anymore, but a single integer that is the sum of two dice. Next we have to make sideA but taking the top four rows, arranging in descending order, grabbing the second highest number, and then pulling the result as an integer. This is followed by sideB which is just the bottom 6 rows taken by tail, and then finding the median of those rows, and then adding 1 to fit the pset parameters.

#Finally, we need to read in the results somewhere and also show the winner for each roll. So we use the results command to "i", which in turn cooresponds to the i in our "for" command. Then we feed this into a case_when command that has three components. The first reads that Side A wins when sideA is greater than sideB. Side B wins when the opposite is true. And when anything else other than that happens (i.e. they are equal), it reads as "Tie"

game <- function(n) {
  results <- vector(length = n)
  for (i in 1:n) {
    x3 <- tibble(throws = map_int(1:10, ~ dice(n = 1)))

    sideA <- x3 %>%
      head(4) %>%
      arrange(desc(throws)) %>%
      slice(2) %>%
      pull(1)

    sideB <- x3 %>%
      tail(6) %>%
      summarise(med = median(throws) + 1) %>%
      pull(1)

    results[i] <- case_when(
      sideA > sideB ~ "Side A",
      sideB > sideA ~ "Side B",
      TRUE ~ "Tie"
    )
  }
  return(results)
}

#To answer the actual Madlibs though we can use the tabyl command from janitor to make this easy. This creates a basic table thats shows the percentages of our three outcomes and with the adorn_pct_formatting command it makes it super nice looking. 

f <- tabyl(game(1000)) %>%
  adorn_pct_formatting()

#The first madlib just requires us selecting the column that includes both Side A and Side B. It's clear that Side B is the more likely scenario and since it's in the second row, we can just use slice to grab it. 

winner <- f %>%
  select("game(1000)") %>%
  slice(2)

#Similar to the last section, the percentage chance of a Tie happening is the third row under our percent column, so I use slice to grab it for the Madlib. 

tie <- f %>%
  select("percent") %>%
  slice(3)
```
`r winner` is  more likely to win.

The odds of a tie are approximately `r tie`.

## Cooperation
I worked with Bernadette Stadler and Dan Shapiro