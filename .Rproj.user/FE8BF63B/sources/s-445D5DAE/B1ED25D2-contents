---
title: 'Problem Set #4'
author: "Cian Stryker"
date: "10/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Various loading of packages is here. I've used the clean and setup packages like readr, janitor, and styler. Also I'm using my favorite graph package cowplot so I can make a cool 4 graph on one pane graph. Other than that I use packages when I needed them for doing the p set. moderndive, data.table, lubridate, and gt are the examples of this. Lubridate is amazing for handling dates.

library(readr)
library(readxl)
library(janitor)
library(moderndive)
library(data.table)
library(cowplot)
library(reprex)
library(gt)
library(lubridate)
library(styler)
library(tidyverse)
```
## Question 1 
```{r, Loading in Question 1, message=FALSE, warning=FALSE}

# Not a big surprise, but you gotta load in data to use it. I saved this data to my "Polls" folder and cleaned up the names. No more cleaning needed though. So that's nice.

polls_1 <- read_csv("Polls/elections-poll-nc09-3.csv") %>%
  clean_names()
```

```{r, First Madlib}

# First Madlib here. Pretty easy, but that doesn't last long in this p set. I just had to filter out what I wanted and then used the old nrow() function to pull in all together.

x <- polls_1 %>%
  select("response", "race_edu") %>%
  filter(response == "Dem") %>%
  filter(race_edu == "Nonwhite") %>%
  nrow()
```

The number of respondents who provide a response of Dem and have a race_edu value of Nonwhite is `r x`.

```{r, Second Madlib}

# And now the madlibs get a little harder. Here I had to first just grab what I wanted to look at, which was partyid and timestamp.

q_2 <- polls_1 %>%
  select("partyid", "timestamp")

# But low and behold the timestamp column isn't actually registred as type "date"..... so thanks for that. I had to shift it over to a date format before R would arrnage it properly. I used lubridate to that and then just filtered for Republican before arranging by timestamp. I didn't use desc() here because I wanted the first respondents.

q_2x <- q_2 %>%
  mutate(timestamp = ymd_hms(timestamp)) %>%
  filter(partyid == "Republican") %>%
  arrange(timestamp)

# Here I just grabbed the first response.

time <- q_2x %>%
  select("timestamp") %>%
  head(1)

# Then I had to register it in this specific date format again so that I could try and loose the date portion and just keep the time. Again lubridate is awesome.

x <- ymd_hms("2018-10-26 22:28:00")

# Finally, using lubridate I was able to grab only the time and add a nice "PM" bit to the end to finish the madlib. Also I used I instead of H so that it doesn't read in military time.

x2 <- format(x, "%I:%M:%S PM")
```
The first respondent with a partyid of Republican came in at `r x2`. 
```{r, Third Madlib}

# At first this madlib was scary, but then I realized I just had to follow the prevous procedure with an extra step at the end. So my first steps were to grab what I wanted, shift timestamp to date form, and then filter out only Dem and Rep. Easy work so far.

q_3 <- polls_1 %>%
  select("response", "timestamp") %>%
  mutate(timestamp = ymd_hms(timestamp)) %>%
  filter(response %in% c("Dem", "Rep"))

# Now I wanted to grab just the Dem responses, find the earliest response, and then grab only that response. To subtract the two from eachtoher I wante to separate them into separate dataframes.

step_1 <- q_3 %>%
  filter(response == "Dem") %>%
  arrange(timestamp) %>%
  head(1)

# Same as the code above. Nothing different at all, but it was necessary to separate them from eachother.

step_2 <- q_3 %>%
  filter(response == "Rep") %>%
  arrange(desc(timestamp)) %>%
  head(1)

# Now I had to register them as proper dates again by using the lubridate package. I'm not sure why I had to use lubridate twice, probably some mistake on my party, but this is how it worked out for me.

date1 <- ymd_hms("2018-10-26 22:06:37")
date2 <- ymd_hms("2018-10-31 00:44:26")

# Now I did two steps. First I used base r to just subtract the two data frames from eachother now that they're in date form. I got the answer in 5917.817 mintues, but calling just "x3" included the words "Time difference of 5917.817 minutes, which I didn't want.

x3 <- difftime(date2, date1, units = "mins")

# Maybe this is cheap, but I didn't know what else to do. I just made a dataframe out of the rounded answer of the last bit of code and then made it register to x4. I'm sure there's a cleaner way to do it, but this was really quick and the problem set is big.

x4 <- data.frame("5918")
```

The number of minutes which elapsed between the first response for Dem and the last response for Rep was `r x4`. 

```{r, Loading for Question 2, message=FALSE, warning=FALSE}

# Classic loading chunk. I never know what to say about these.... but the date is in my polls folder after this. Oh and some name cleaning is always nice.

polls_2 <- read_csv("Polls/elections-poll-ia03-3.csv") %>%
  clean_names()
```
## Question 2

```{r, Question 2}

# This was really difficult and I eventually got help from Bernadette who attended a study hall to tackle this question. I'm going to walk through my steps here though. So first I had to load in my data and grab just the info that I wanted, i.e. likley, response, and final_weight. Then I filtered our the Don't know and Refused responses, because we don't need them here. Then its time to group likely and response together so we can begin reorganzing the final_weight column without loosing its coorespondense to the two orignal columns.

# Now it gets interesting. So the key is to change the final_weight column to a new column in which for every grouping of response and likley, there is only the sum of that type's final weight. This allows us to calculate later. First, though, we need to shift that data from long to wide form via spread. Then we need to create something we can actually divide by to find the percentages, so it's important to add each column's value across its rows to create the total repsonse column. Then we simply use mutate on each of our target demographics (Dem, Rep, Und) to find it's percentage. Note: filling in the missing data with NA's in each of these stages and teh previous two is key to adding in a "---" later in GT. Finally, to end it all, we just select our targets and then ungroup to prep them for graphing.

table_data <- polls_2 %>%
  select(likely, response, final_weight) %>%
  filter(
    !likely %in% c("[DO NOT READ] Don't know/Refused", "[DO NOT READ] Refused")
  ) %>%
  group_by(likely, response) %>%
  summarise(weights = sum(final_weight)) %>%
  spread(key = response, value = weights, fill = NA_character_) %>%
  mutate(total = sum(`3`, `4`, `5`, `6`, Dem, Rep, Und, na.rm = TRUE)) %>%
  mutate(Dem = Dem / total, na.rm = TRUE) %>%
  mutate(Rep = Rep / total, na.rm = TRUE) %>%
  mutate(Und = Und / total, na.rm = TRUE) %>%
  select(likely, Dem, Rep, Und) %>%
  ungroup()


# Here we need to reorder the rows in the same way they were for the NYT's table. To do this we create a value of orders in the same way as the NYT's.

reorder <- c("Already voted", "Almost certain", "Very likely", "Somewhat likely", "Not very likely", "Not at all likely")

# Now using slice and match, we are combining the value we created before to align with our table_data so that the information is arranged in the proper way.

table_data_reordered <- table_data %>% slice(match(reorder, likely))

# Now it's gt graphing time. Nothing particularly crazy here but of note is the use of fmt_percent and decimals = 0 to add percent signs in and remove the decimals we don't need. We remove the column label for the first column to match, add in our titles, subtitles, and note for style and then we fill in the NA's with a dash. Of final note is realigning the columns to center to complete the aesthetic appeal.

gt(table_data_reordered) %>%
  fmt_percent(vars(Dem, Rep, Und), decimals = 0) %>%
  cols_label(likely = " ") %>%
  tab_header(
    title = "Intention of Voting",
    subtitle = "Iowa 3rd District, 2018 Midterm Election"
  ) %>%
  fmt_missing(columns = 2:4, rows = NULL, missing_text = "---") %>%
  cols_align(align = "center") %>%
  tab_source_note(
    source_note = "Data from the New York Time's 2018 Election Polling"
  )
```


## Question 3
```{r, Prep and Graph for Question 3}

# This question was harder than usual, but easy once I read the chapters that coorelated. Step one was to make my bowl. I just make an ID value of 1-5000, and then another value of bead with a ratio of 1 crimson to 4 white.

ID <- c(1:5000)
bead <- c("crimson", "white", "white", "white", "white")

# Big surprise. I combined my two values together to create the super useful virtual "bowl" of my beads.

bowl <- data.frame(ID, bead)

# The next step was to use the rep_sample_n() function to grab 25 random observations from bowl, 5000 times. By the way, this is actually super cool to me.

virtual_samples <- bowl %>%
  rep_sample_n(size = 25, reps = 5000)

# Now here comes the tricker party, but I essentially copied the code from the reading. Here I can group by replicate, create a sum of crimson beads per replication, and then find the proportion that is crimson, registered as a new variable. Thank you to our readings for this.

virtual_prop_red <- virtual_samples %>%
  group_by(replicate) %>%
  summarize(crimson = sum(bead == "crimson")) %>%
  mutate(prop_crimson = crimson / 25)

# Finally it's graphing time. At first I made it pretty simple with just the ggplot() and geom_histogram() portions. Plus I copied the reading's binwidth and boundary specifications. Then I made it a little fancier by filling in the histogram with the color red. Do we have access to a crimson color? I don't know, but red is close enough. I also added in the titles I needed to. I then removed the legend from fill and color since they weren't necessary.

ggplot(virtual_prop_red, aes(x = prop_crimson, color = "red", fill = "red")) +
  geom_histogram(binwidth = 0.05, boundary = 0.4) +
  labs(
    x = "Proportion of 5000 balls that were Crimson",
    title = "Distribution of 25 proportions of Crimson"
  ) +
  guides(color = FALSE, fill = FALSE)
```

## Question 4
```{r, Sampling for Question 4, message=FALSE}

# Alright I'm going to be honest here. I wanted to copy Preceptor Kane's graph because I thought it was cool, but I initially wrote the code four times. I just wanted to get the graph made first, before I went back through and simplified my code and used purrr. But I'm going to procede from here acting like I always used my own function and purrr techniques.

# So I just made my own function run the sampling code that I ran in the prevous question. By setting n as the size and the divisible portion, I made it flexible enought to work later on for me.

test <- function(n) {
  bowl %>%
    rep_sample_n(size = n, reps = 1000) %>%
    group_by(replicate) %>%
    summarize(crimson = sum(bead == "crimson")) %>%
    mutate(prop_crimson = crimson / n) %>%
    select(prop_crimson)
}
# Here I made a list of my function ran four times. This means that my four tests are in four datasets within one larger list. From here I was able to begin using purrr.

func <- list(test(50), test(100), test(500), test(2500))

# I'm not sure I even like purrr but I had to use it here so whatever. First I used the map function to apply the data.frame command to each of my tests within my list. Then I did the same with melt, so that all the observations would be in the correct long from. Then I had to use the rbindlist() command to combine the four tests into one dataframe with a new column that differentiated them from eachother. In this case the id coloumn was literally id.

# Then I used the as.factor() command record them as the right type for my graphing. Originally I did this without purrr, but I see why we should get some purrr practice.

ii <- map(func, data.frame)
uu <- map(ii, reshape2::melt)
ww <- rbindlist(uu, idcol = TRUE)
ww$.id <- as.factor(ww$.id)
```

```{r, Graphing for Question 4}

# Graphing time! Nothing crazy here, but I realized that the Preceptor's graph was actually a collection of density graphs. At least I think it is... but I used densities myself and then just renamed the y axis to match his. I added a bunch of labels to match, and made it alpha to create that cool effect where you can see each distribution stacked on eachother. I did not copy the colors though because I wanted a degree of originality. Oh, setting the seq, and lim within the scale_x_continous() is really important to get the overall look right.

ggplot(ww, aes(x = value, group = .id, fill = .id)) +
  geom_density(alpha = .3) +
  labs(
    x = "Proportion of balls that were crimson",
    title = "Distribution of Percentage Crimson",
    subtitle = "Average sampled percentage is close to the true percentage of 20%",
    y = "Count"
  ) +
  scale_x_continuous(breaks = seq(0, .5, 0.1), lim = c(0, .5)) +
  guides(color = FALSE) +
  scale_fill_discrete(name = "Size", labels = c("50", "100", "500", "2500"))
```

## Question 7
The workload estimate from the reviews of GOV 1005 present a few issues we should examine more closely. We have information for the parameters that would suggest this score is representative of overall opinion from the class. In terms of the population, we know that the responses are from students who have completed GOV 1005. The population pararemters are the average hours they put into the class per week. The "census" in this case is Haravrd or the Preceptor sending evaluations to each student and asking them to record their average workload per week. The sampel statistic is simply how many students recorded their work to be <3 hours, between 3-6, between 6-10, between 11-4 hours, or those who said they worked more than 14 hours. These areas of sampling are all accounted for, but other areas are less certain. 

These areas include whether it is representative, generalizable, and unbiased. Here the fundamental flaw is that these evaluations are not randomized. Only students who want to fill out the evaluation do so, so we have no way to know if it is truly unbiased. We also know that it may not be representative or generalizable because it wasn't randomized. Knowing this, I would hesitate to say the workload is really representative of the class. The self-selection issue here means that it's likely only students who either really enjoyed the course or really disliked the course actually filled out the evaluations. This may then have an effect on the workload as I can imagine students who dislike the course might have worked more on it. That being said, I would also guess that the workload statistic is actually fairly accurate. This is because the other evaluations scores show that students overwhelmingly enjoy the course, but the workload is still fairly high. This would suggest a lot of students who enjoyed the course, filled out the evaluations, and then recorded that their workload per week was high. This does not, however, remove the flaws with the sampling overall. 


## Cooperation

I worked with Bernadette Stadler again. 




